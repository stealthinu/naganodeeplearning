{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** 誤差関数と勾配降下法の考えを導入しアナログ値対応します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** 活性化関数、出力の式、学習の式を修正します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1** シグモイド関数を作ってグラフを表示してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dsigmoid(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.2** 活性化関数と学習式の変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dh(x):\n",
    "    return dsigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写経** シグモイド関数とシグモイド関数の微分、活性化関数と活性化関数の微分のコードを書きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' 元のパーセプトロンの内容\n",
    "def output_perceptron(w, x):\n",
    "    s = 0.0\n",
    "    for i in range(len(w)):\n",
    "        s += w[i] * x[i]\n",
    "    y = h(s - b)\n",
    "    return y\n",
    "'''\n",
    "def output_perceptron(w, x):\n",
    "    s = np.sum(w*x)\n",
    "    y = h(s - b)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写経** 修正したコードを写経してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' 元のパーセプトロンの内容\n",
    "def train_perceptron(w, x, t):\n",
    "    y = output_perceptron(w, x)\n",
    "    for i in range(len(w)):\n",
    "        dw = alpha*(t - y)*x[i]\n",
    "        w[i] += dw\n",
    "    square_error = np.square(t - y)\n",
    "    return square_error\n",
    "'''\n",
    "def train_perceptron(w, x, t):\n",
    "    o = output_perceptron(w, x)\n",
    "    delta = alpha*(t - o)*dh(o);\n",
    "    w += delta*x\n",
    "    square_error = np.square(t - o)\n",
    "    return square_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写経** 修正したコードを写経してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neurons_w = np.random.rand(10, 28*28) * 0.1\n",
    "b = 0.1\n",
    "alpha = 0.01\n",
    "train_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mnist(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mnist(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "train_mnist(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mnist_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** バックプロパゲーションの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1** クラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size                           # 入力数\n",
    "        self.output_size = output_size                         # ニューロン数と同じ\n",
    "        self.w = (np.random.rand(output_size, input_size) - 0.5) * 0.1 # ニューロンごとに入力と同数の足がある分の重み\n",
    "        self.b = (np.random.rand(output_size) - 0.5) * 0.2             # ニューロンごとのしきい値\n",
    "    \n",
    "    def h(self, x):\n",
    "        return sigmoid(x)\n",
    "    \n",
    "    def dh(self, x):\n",
    "        return dsigmoid(x)\n",
    "    \n",
    "    def output_neuron(self, W, b, X):\n",
    "        s = np.sum(W*X)\n",
    "        y = self.h(s - b)\n",
    "        return y\n",
    "    \n",
    "    def train_neuron(self, W, b, X, T):\n",
    "        O = self.output_neuron(W, b, X)\n",
    "        d = -alpha*(T - O)*self.dh(O);\n",
    "        W -= d*X\n",
    "        return\n",
    "    \n",
    "    def output_layer(self, x):\n",
    "        y = np.zeros(self.output_size)\n",
    "        for i in range(self.output_size):\n",
    "            y[i] = self.output_neuron(self.w[i], self.b[i], x)\n",
    "        return y\n",
    "\n",
    "    def train_layer(self, x, t):\n",
    "        for i in range(self.output_size):\n",
    "            self.train_neuron(self.w[i], self.b[i], x, t[i])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer1 = Layer(784, 10)\n",
    "alpha = 0.1\n",
    "\n",
    "def train_mnist(train_count):\n",
    "    images, labels = mnist.train.next_batch(train_count)\n",
    "    square_error = 0.0\n",
    "    for i in range(len(images)):\n",
    "        x = images[i]\n",
    "        t = labels[i]\n",
    "        y = layer1.output_layer(x)\n",
    "        layer1.train_layer(x, t)\n",
    "        square_error += calc_square_error(y, t)\n",
    "    print(square_error/train_count/10) # 1ニューロンあたりの誤差\n",
    "\n",
    "train_mnist(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mnist(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_mnist():\n",
    "    images, labels = mnist.test.next_batch(1)\n",
    "    x = images[0]\n",
    "    t = labels[0]\n",
    "    y = layer1.output_layer(x)\n",
    "    print(t)\n",
    "    print(y)\n",
    "    print(np.argmax(y))\n",
    "    # イメージ表示\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.imshow(x.reshape((28,28)), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1** ニューロンごとの誤差の元を保持する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size                           # 入力数\n",
    "        self.output_size = output_size                         # ニューロン数と同じ\n",
    "        self.neurons_W = (np.random.rand(output_size, input_size) - 0.5) * 0.1 # ニューロンごとに入力と同数の足がある分の重み\n",
    "        self.neurons_b = (np.random.rand(output_size) - 0.5) * 0.2             # ニューロンごとのしきい値\n",
    "        self.neurons_d = np.zeros(output_size)                                 # ニューロンごとの誤差修正値\n",
    "    \n",
    "    def h(self, x):\n",
    "        return sigmoid(x)\n",
    "        #return tanh(x)\n",
    "        #return relu(x)\n",
    "    \n",
    "    def dh(self, y):\n",
    "        return dsigmoid(y)\n",
    "        #return dtanh(y)\n",
    "        #return drelu(y)\n",
    "    \n",
    "    def output_layer(self, X):\n",
    "        y = np.zeros(self.output_size)\n",
    "        for i in range(self.output_size):\n",
    "            # y = h(sum(W*X) - b)\n",
    "            y[i] = self.h(np.sum(self.neurons_W[i]*X) - self.neurons_b[i])\n",
    "        return y\n",
    "\n",
    "    def train_layer(self, X):\n",
    "        for i in range(self.output_size):\n",
    "            # W = W - (-alpha*d*X)\n",
    "            self.neurons_W[i] -= -alpha*self.neurons_d[i]*X\n",
    "        return\n",
    "    \n",
    "    # 出力層での自分自身の誤差修正値をセット\n",
    "    def backprop_layer_output(self, O, T):\n",
    "        # d = (T - O)*dh(O)\n",
    "        self.neurons_d = (T - O)*self.dh(O)\n",
    "        return\n",
    "\n",
    "    # 前のレイヤーへの誤差修正値を算出\n",
    "    def backprop_layer(self):\n",
    "        total_prev_D = np.zeros(self.input_size)\n",
    "        for i in range(self.output_size):\n",
    "            #total_prev_D += W*d\n",
    "            total_prev_D += self.neurons_W[i]*self.neurons_d[i]\n",
    "        return total_prev_D\n",
    "\n",
    "    # 後のレイヤーで計算された誤差修正値に出力値を掛けて自レイヤーに値をセット\n",
    "    def set_layer_d(self, D, O):\n",
    "        # d = D*dh(O)\n",
    "        self.neurons_d = D*self.dh(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer1 = Layer(784, 10)\n",
    "alpha = 0.01\n",
    "\n",
    "def train_mnist(train_count):\n",
    "    images, labels = mnist.train.next_batch(train_count)\n",
    "    square_error = 0.0\n",
    "    for i in range(len(images)):\n",
    "        X = images[i]\n",
    "        T = labels[i]\n",
    "        Y = layer1.output_layer(X)\n",
    "        layer1.backprop_layer_output(Y, T)\n",
    "        layer1.train_layer(X)\n",
    "        square_error += calc_square_error(Y, T)\n",
    "    print(square_error/train_count/10) # 1ニューロンあたりの誤差\n",
    "\n",
    "train_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "train_mnist(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ３層ネットワーク 784x100x10\n",
    "layer1 = Layer(784, 100)\n",
    "layer2 = Layer(100, 10)\n",
    "alpha = 0.01\n",
    "mini_batch = 100\n",
    "epoch = 0\n",
    "epochs, dpns = [], []\n",
    "\n",
    "def output_backprop_mnist(X):\n",
    "    # 正方向でニューロンの出力\n",
    "    O1 = layer1.output_layer(X)\n",
    "    Y  = layer2.output_layer(O1)\n",
    "    return Y\n",
    "\n",
    "def train_backprop_mnist(train_count):\n",
    "    global layer1, layer2, epoch, epochs, dpns\n",
    "    for count in range(train_count):\n",
    "        images, labels = mnist.train.next_batch(mini_batch)\n",
    "        square_error = 0.0\n",
    "        for i in range(mini_batch):\n",
    "            X = images[i]\n",
    "            T = labels[i]\n",
    "            # 正方向でニューロンの出力\n",
    "            O1 = layer1.output_layer(X)\n",
    "            Y  = layer2.output_layer(O1)\n",
    "            # 逆伝搬で誤差修正値をニューロン毎に算出\n",
    "            layer2.backprop_layer_output(Y, T)\n",
    "            D1 = layer2.backprop_layer()\n",
    "            layer1.set_layer_d(D1, O1)\n",
    "            # 誤差修正値を使って学習\n",
    "            layer2.train_layer(O1)\n",
    "            layer1.train_layer(X)\n",
    "            # 二乗誤差算出\n",
    "            square_error += calc_square_error(Y, T)\n",
    "        epoch += mini_batch\n",
    "        dpn = square_error/mini_batch/10 # 1ニューロンあたりの誤差\n",
    "        epochs.append(epoch)\n",
    "        dpns.append(dpn)\n",
    "    print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "\n",
    "train_backprop_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "train_backprop_mnist(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(epochs, dpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_backprop_mnist():\n",
    "    images, labels = mnist.test.next_batch(1)\n",
    "    X = images[0]\n",
    "    T = labels[0]\n",
    "    Y = output_backprop_mnist(X)\n",
    "    print(T)\n",
    "    print(Y)\n",
    "    print(np.argmax(Y))\n",
    "    # イメージ表示\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.imshow(X.reshape((28,28)), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_backprop_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_backprop_mnist_error():\n",
    "    test_count = 1000\n",
    "    images, labels = mnist.test.next_batch(test_count)\n",
    "    ng = 0\n",
    "    for j in range(len(images)):\n",
    "        X = images[j]\n",
    "        T = labels[j]\n",
    "        Y = output_backprop_mnist(X)\n",
    "        if np.argmax(T) != np.argmax(Y):\n",
    "            ng += 1\n",
    "            #print(\"T:\" + str(np.argmax(T)) + \" Y:\" + str(np.argmax(Y)) + \" NG!\")\n",
    "        #else:\n",
    "            #print(\"T:\" + str(np.argmax(T)) + \" Y:\" + str(np.argmax(Y)))\n",
    "    print(ng / test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_backprop_mnist_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ディープラーニングの技術を試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の高速化手法の変更（ローカルミニマム対策としても）\n",
    "- SGD (素の確率的勾配降下法）\n",
    "- Momentum\n",
    "- AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配消失問題への対応（活性化関数の変更）\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_square_error(O, T):\n",
    "    return np.sum(np.square(O - T))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y*(1 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - np.square(np.tanh(y))\n",
    "\n",
    "def relu(x):\n",
    "    #return x if x > 0 else 0\n",
    "    return x * (x > 0)\n",
    "\n",
    "def drelu(y):\n",
    "    #return 1 if y > 0 else 0\n",
    "    return 1 * (y > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size                           # 入力数\n",
    "        self.output_size = output_size                         # ニューロン数と同じ\n",
    "        self.neurons_W = (np.random.rand(output_size, input_size) - 0.5) * 0.1 # ニューロンごとに入力と同数の足がある分の重み\n",
    "        self.neurons_b = (np.random.rand(output_size) - 0.5) * 0.2             # ニューロンごとのしきい値\n",
    "        self.neurons_d = np.zeros(output_size)                                 # ニューロンごとの誤差修正値\n",
    "        self.neurons_P = np.zeros((output_size, input_size))                   # パラメータ更新の高速化用保持パラメータ\n",
    "\n",
    "    def h(self, x):\n",
    "        return sigmoid(x)\n",
    "        #return tanh(x)\n",
    "        #return relu(x)\n",
    "    \n",
    "    def dh(self, y):\n",
    "        return dsigmoid(y)\n",
    "        #return dtanh(y)\n",
    "        #return drelu(y)\n",
    "    \n",
    "    def dw(self, i, X):\n",
    "        #return self.sgd(i, X)\n",
    "        return self.momentum(i, X)\n",
    "        #return self.adagrad(i, X)\n",
    "    \n",
    "    def output_layer(self, X):\n",
    "        y = np.zeros(self.output_size)\n",
    "        for i in range(self.output_size):\n",
    "            # y = h(sum(W*X) - b)\n",
    "            y[i] = self.h(np.sum(self.neurons_W[i]*X) - self.neurons_b[i])\n",
    "        return y\n",
    "\n",
    "    def train_layer(self, X):\n",
    "        for i in range(self.output_size):\n",
    "            # W = W - (-alpha*d*X)\n",
    "            #self.neurons_W[i] -= -alpha*self.neurons_d[i]*X\n",
    "            # W = W - dw(d, X)\n",
    "            self.neurons_W[i] -= self.dw(i, X)\n",
    "        return\n",
    "    \n",
    "    # 出力層での自分自身の誤差修正値をセット\n",
    "    def backprop_layer_output(self, O, T):\n",
    "        # d = (T - O)*dh(O)\n",
    "        self.neurons_d = (T - O)*self.dh(O)\n",
    "        return\n",
    "\n",
    "    # 前のレイヤーへの誤差修正値を算出\n",
    "    def backprop_layer(self):\n",
    "        total_prev_D = np.zeros(self.input_size)\n",
    "        for i in range(self.output_size):\n",
    "            #total_prev_D += W*d\n",
    "            total_prev_D += self.neurons_W[i]*self.neurons_d[i]\n",
    "        return total_prev_D\n",
    "\n",
    "    # 後のレイヤーで計算された誤差修正値に出力値を掛けて自レイヤーに値をセット\n",
    "    def set_layer_d(self, D, O):\n",
    "        # d = D*dh(O)\n",
    "        self.neurons_d = D*self.dh(O)\n",
    "\n",
    "    # 学習の高速化比較\n",
    "    def sgd(self, i, X):\n",
    "        # W = W - alpha*d*X\n",
    "        return -alpha*self.neurons_d[i]*X\n",
    "\n",
    "    def momentum(self, i, X):\n",
    "        # v = momentum*v - alpha*d*X\n",
    "        # W = W + v\n",
    "        momentum = 0.9\n",
    "        self.neurons_P[i] = momentum*self.neurons_P[i] - alpha*self.neurons_d[i]*X\n",
    "        return self.neurons_P[i]\n",
    "\n",
    "    def adagrad(self, i, X):\n",
    "        # h = h + (d*X)*(d*X)\n",
    "        # W = W - alpha*(1/sqrt(h))*d*X\n",
    "        self.neurons_P[i] += (self.neurons_d[i]*X)*(self.neurons_d[i]*X)\n",
    "        return -alpha*self.neurons_d[i]*X/(np.sqrt(self.neurons_P[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ３層ネットワーク 784x100x10\n",
    "layer1 = Layer(784, 100)\n",
    "layer2 = Layer(100, 10)\n",
    "alpha = 0.01\n",
    "mini_batch = 100\n",
    "epoch = 0\n",
    "epochs, dpns = [], []\n",
    "\n",
    "def output_backprop_mnist(X):\n",
    "    # 正方向でニューロンの出力\n",
    "    O1 = layer1.output_layer(X)\n",
    "    Y  = layer2.output_layer(O1)\n",
    "    return Y\n",
    "\n",
    "def train_backprop_mnist(train_count):\n",
    "    global layer1, layer2, epoch, epochs, dpns\n",
    "    for count in range(train_count):\n",
    "        images, labels = mnist.train.next_batch(mini_batch)\n",
    "        square_error = 0.0\n",
    "        for i in range(mini_batch):\n",
    "            X = images[i]\n",
    "            T = labels[i]\n",
    "            # 正方向でニューロンの出力\n",
    "            O1 = layer1.output_layer(X)\n",
    "            Y  = layer2.output_layer(O1)\n",
    "            # 逆伝搬で誤差修正値をニューロン毎に算出\n",
    "            layer2.backprop_layer_output(Y, T)\n",
    "            D1 = layer2.backprop_layer()\n",
    "            layer1.set_layer_d(D1, O1)\n",
    "            # 誤差修正値を使って学習\n",
    "            layer2.train_layer(O1)\n",
    "            layer1.train_layer(X)\n",
    "            # 二乗誤差算出\n",
    "            square_error += calc_square_error(Y, T)\n",
    "        epoch += mini_batch\n",
    "        dpn = square_error/mini_batch/10 # 1ニューロンあたりの誤差\n",
    "        epochs.append(epoch)\n",
    "        dpns.append(dpn)\n",
    "    print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "\n",
    "train_backprop_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "train_backprop_mnist(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(epochs, dpns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_sigmoid_sgd = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_sigmoid_momentum = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_sigmoid_adagrad = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(epochs, dpns_sigmoid_sgd)\n",
    "plt.plot(epochs, dpns_sigmoid_momentum)\n",
    "plt.plot(epochs, dpns_sigmoid_adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層でも学習が収束するための手法（ローカルミニマム対策）\n",
    "- PreTraining（事前学習）\n",
    "- DropOut\n",
    "- バッチ正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ６層ネットワーク 784x160x80x40x20x10\n",
    "layer1 = Layer(784, 160)\n",
    "layer2 = Layer(160, 80)\n",
    "layer3 = Layer(80,  40)\n",
    "layer4 = Layer(40,  20)\n",
    "layer5 = Layer(20,  10)\n",
    "alpha = 0.01\n",
    "mini_batch = 100\n",
    "epoch = 0\n",
    "epochs, dpns = [], []\n",
    "\n",
    "def output_backprop_mnist(X):\n",
    "    # 正方向でニューロンの出力\n",
    "    O1 = layer1.output_layer(X)\n",
    "    O2 = layer2.output_layer(O1)\n",
    "    O3 = layer3.output_layer(O2)\n",
    "    O4 = layer4.output_layer(O3)\n",
    "    Y  = layer5.output_layer(O4)\n",
    "    return Y\n",
    "\n",
    "def train_backprop_mnist(train_count):\n",
    "    global layer1, layer2, layer3, layer4, layer5, epoch, epochs, dpns\n",
    "    for count in range(train_count):\n",
    "        images, labels = mnist.train.next_batch(mini_batch)\n",
    "        square_error = 0.0\n",
    "        for i in range(mini_batch):\n",
    "            X = images[i]\n",
    "            T = labels[i]\n",
    "            # 正方向でニューロンの出力\n",
    "            O1 = layer1.output_layer(X)\n",
    "            O2 = layer2.output_layer(O1)\n",
    "            O3 = layer3.output_layer(O2)\n",
    "            O4 = layer4.output_layer(O3)\n",
    "            Y  = layer5.output_layer(O4)\n",
    "            # 逆伝搬で誤差修正値をニューロン毎に算出\n",
    "            layer5.backprop_layer_output(Y, T)\n",
    "            D4 = layer5.backprop_layer()\n",
    "            layer4.set_layer_d(D4, O4)\n",
    "            D3 = layer4.backprop_layer()\n",
    "            layer3.set_layer_d(D3, O3)\n",
    "            D2 = layer3.backprop_layer()\n",
    "            layer2.set_layer_d(D2, O2)\n",
    "            D1 = layer2.backprop_layer()\n",
    "            layer1.set_layer_d(D1, O1)\n",
    "            # 誤差修正値を使って学習\n",
    "            layer5.train_layer(O4)\n",
    "            layer4.train_layer(O3)\n",
    "            layer3.train_layer(O2)\n",
    "            layer2.train_layer(O1)\n",
    "            layer1.train_layer(X)\n",
    "            # 二乗誤差算出\n",
    "            square_error += calc_square_error(Y, T)\n",
    "        epoch += mini_batch\n",
    "        dpn = square_error/mini_batch/10 # 1ニューロンあたりの誤差\n",
    "        epochs.append(epoch)\n",
    "        dpns.append(dpn)\n",
    "    print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "\n",
    "train_backprop_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_backprop_mnist(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数と重みの変更方法を変えてそれぞれ算出してグラフにしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_6layer_sigmoid_sgd = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_6layer_sigmoid_momentum = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_6layer_tanh_sgd = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_6layer_tanh_momentum = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpns_6layer_relu_momentum = dpns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(epochs, dpns_6layer_sigmoid_sgd, label=\"sigmoid_sgd\")\n",
    "plt.plot(epochs, dpns_6layer_sigmoid_momentum, label=\"sigmoid_momentum\")\n",
    "plt.plot(epochs, dpns_6layer_tanh_sgd, label=\"tanh_sgd\")\n",
    "plt.plot(epochs, dpns_6layer_tanh_momentum, label=\"tanh_momentum\")\n",
    "plt.plot(epochs, dpns_6layer_relu_momentum, label=\"relu_momentum\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smaller(image):\n",
    "    def seri(x, y):\n",
    "        return y*28 + x\n",
    "\n",
    "    simages = np.zeros(14*14)\n",
    "    for i in range(14):\n",
    "        for j in range(14):\n",
    "            x, y = j*2, i*2\n",
    "            avg = (image[seri(x+0, y+0)] + image[seri(x+0, y+1)] + image[seri(x+1, y+0)] + image[seri(x+1, y+1)])/4.0\n",
    "            simages[i*14 + j] = avg\n",
    "    return simages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PreTraining ６層ネットワーク 784x160x80x40x20x10\n",
    "#layer1 = Layer(196, 100) # 14x14 (28x28の1/4)\n",
    "#layer2 = Layer(100, 60)\n",
    "#layer3 = Layer(60,  40)\n",
    "#layer4 = Layer(40,  20)\n",
    "#layer5 = Layer(20,  10)\n",
    "# 元に戻すためのレイヤー\n",
    "#layer1r = Layer(100, 196)\n",
    "#layer2r = Layer(60, 100)\n",
    "#layer3r = Layer(40, 60)\n",
    "#layer4r = Layer(20, 40)\n",
    "#layer5r = Layer(10, 20)\n",
    "alpha = 0.01\n",
    "mini_batch = 100\n",
    "epoch = 0\n",
    "epochs, dpns = [], []\n",
    "\n",
    "def output_pretrain_mnist(X):\n",
    "    # 正方向でニューロンの出力\n",
    "    O1 = layer1.output_layer(X)\n",
    "    O2 = layer2.output_layer(O1)\n",
    "    O3 = layer3.output_layer(O2)\n",
    "    O4 = layer4.output_layer(O3)\n",
    "    Y  = layer5.output_layer(O4)\n",
    "#    O5 = layer5.output_layer(O4)\n",
    "#    Y  = layer5r.output_layer(O5)\n",
    "    return Y\n",
    "\n",
    "def train_pretrain_mnist(train_count):\n",
    "    global layer1, layer2, layer3, layer4, layer5, epoch, epochs, dpns\n",
    "    global layer1r, layer2r, layer3r, layer4r, layer5r\n",
    "    for count in range(train_count):\n",
    "        images, labels = mnist.train.next_batch(mini_batch)\n",
    "        square_error = 0.0\n",
    "        for i in range(mini_batch):\n",
    "            X = smaller(images[i])\n",
    "            T = labels[i]\n",
    "            # 正方向でニューロンの出力\n",
    "            O1 = layer1.output_layer(X)\n",
    "            O2 = layer2.output_layer(O1)\n",
    "            O3 = layer3.output_layer(O2)\n",
    "            O4 = layer4.output_layer(O3)\n",
    "            Y  = layer5.output_layer(O4)\n",
    "            #O5 = layer5.output_layer(O4)\n",
    "            #Y  = layer5r.output_layer(O5)\n",
    "            # 逆伝搬で誤差修正値をニューロン毎に算出\n",
    "            layer5.backprop_layer_output(Y, T)\n",
    "            D4 = layer5.backprop_layer()\n",
    "            layer4.set_layer_d(D4, O4)\n",
    "            D3 = layer4.backprop_layer()\n",
    "            layer3.set_layer_d(D3, O3)\n",
    "            D2 = layer3.backprop_layer()\n",
    "            layer2.set_layer_d(D2, O2)\n",
    "            D1 = layer2.backprop_layer()\n",
    "            layer1.set_layer_d(D1, O1)\n",
    "            #layer5r.backprop_layer_output(Y, O4)\n",
    "            #D = layer5r.backprop_layer()\n",
    "            #layer5.set_layer_d(D, O5)\n",
    "            # 誤差修正値を使って学習\n",
    "            layer5.train_layer(O4)\n",
    "            layer4.train_layer(O3)\n",
    "            layer3.train_layer(O2)\n",
    "            layer2.train_layer(O1)\n",
    "            layer1.train_layer(X)\n",
    "            #layer5r.train_layer(O5)\n",
    "            #layer5.train_layer(O4)\n",
    "            # 二乗誤差算出\n",
    "            square_error += calc_square_error(Y, T)\n",
    "#            square_error += calc_square_error(Y, O4)\n",
    "        epoch += mini_batch\n",
    "        dpn = square_error/mini_batch/10 # 1ニューロンあたりの誤差\n",
    "        epochs.append(epoch)\n",
    "        dpns.append(dpn)\n",
    "        print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "    print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "\n",
    "def test_pretrain():\n",
    "    images, labels = mnist.test.next_batch(1)\n",
    "    X = smaller(images[0])\n",
    "    T = labels[0]\n",
    "    Y = output_pretrain_mnist(X)\n",
    "    # イメージ表示\n",
    "    #fig = plt.figure(figsize=(8,4))\n",
    "    fig, (figL, figR) = plt.subplots(ncols=2, figsize=(8,4))\n",
    "    figL.imshow(X.reshape((14,14)), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    figR.imshow(Y.reshape((14,14)), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "\n",
    "train_pretrain_mnist(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pretrain_mnist(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pretrain_mnist():\n",
    "    images, labels = mnist.test.next_batch(1)\n",
    "    X = smaller(images[0])\n",
    "    T = labels[0]\n",
    "    Y = output_pretrain_mnist(X)\n",
    "    print(T)\n",
    "    print(Y)\n",
    "    print(np.argmax(Y))\n",
    "    # イメージ表示\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    plt.imshow(X.reshape((14,14)), vmin=0, vmax=1,cmap=plt.cm.gray_r, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pretrain_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_pretrain_mnist_error():\n",
    "    test_count = 1000\n",
    "    images, labels = mnist.test.next_batch(test_count)\n",
    "    ng = 0\n",
    "    for j in range(len(images)):\n",
    "        X = smaller(images[j])\n",
    "        T = labels[j]\n",
    "        Y = output_pretrain_mnist(X)\n",
    "        if np.argmax(T) != np.argmax(Y):\n",
    "            ng += 1\n",
    "            #print(\"T:\" + str(np.argmax(T)) + \" Y:\" + str(np.argmax(Y)) + \" NG!\")\n",
    "        #else:\n",
    "            #print(\"T:\" + str(np.argmax(T)) + \" Y:\" + str(np.argmax(Y)))\n",
    "    print(ng / test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pretrain_mnist_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XORの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_xor    = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "teacher_xor = np.array([[0.2],  [0.8],  [0.8],  [0.2]])\n",
    "\n",
    "# ３層ネットワーク 2x4x1\n",
    "layer1 = Layer(2, 4)\n",
    "layer2 = Layer(4, 1)\n",
    "alpha = 0.05\n",
    "mini_batch = 1\n",
    "epoch = 0\n",
    "epochs, dpns = [], []\n",
    "\n",
    "\n",
    "def output_backprop_xor(X):\n",
    "    # 正方向でニューロンの出力\n",
    "    O1 = layer1.output_layer(X)\n",
    "    Y  = layer2.output_layer(O1)\n",
    "    return Y\n",
    "\n",
    "def train_backprop_xor(train_count):\n",
    "    global layer1, layer2, epoch, epochs, dpns\n",
    "    for count in range(train_count):\n",
    "        square_error = 0.0\n",
    "        for batch_i in range(mini_batch):\n",
    "            i = np.random.randint(4)\n",
    "            X = data_xor[i]\n",
    "            T = teacher_xor[i]\n",
    "            # 正方向でニューロンの出力\n",
    "            O1 = layer1.output_layer(X)\n",
    "            Y  = layer2.output_layer(O1)\n",
    "            # 逆伝搬で誤差修正値をニューロン毎に算出\n",
    "            layer2.backprop_layer_output(Y, T)\n",
    "            D1 = layer2.backprop_layer()\n",
    "            layer1.set_layer_d(D1, O1)\n",
    "            # 誤差修正値を使って学習\n",
    "            layer2.train_layer(O1)\n",
    "            layer1.train_layer(X)\n",
    "            # 二乗誤差算出\n",
    "            square_error += calc_square_error(Y, T)\n",
    "        epoch += mini_batch\n",
    "        dpn = square_error/mini_batch/1 # 1ニューロンあたりの誤差\n",
    "        epochs.append(epoch)\n",
    "        dpns.append(dpn)\n",
    "    print(\"epoch:\" + str(epoch) + \" d/neuron:\" + str(dpn))\n",
    "\n",
    "def test_backprop_xor():\n",
    "    for i in range(4):\n",
    "        X = data_xor[i]\n",
    "        T = teacher_xor[i]\n",
    "        Y = output_backprop_xor(X)\n",
    "        print(\"X:\" + str(X) + \" Y:\" + str(Y) + \" T:\" + str(T))\n",
    "\n",
    "train_backprop_xor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_backprop_xor(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_backprop_xor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "def test2(x):\n",
    "    test1(x[0])\n",
    "def test1(x):\n",
    "    #x = x + np.array([2, 3])\n",
    "    x += np.array([2, 3])\n",
    "test2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は作業中メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cProfile.run('train_pretrain_mnist(100)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3 install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "layer1.train_layer = MethodType(train_layer, layer1, Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "Tracer()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
